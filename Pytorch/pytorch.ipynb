{"cells":[{"metadata":{},"cell_type":"markdown","source":"# 2.2 数据操作"},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\n\ntorch.manual_seed(0)\ntorch.cuda.manual_seed(0)\nprint(torch.__version__)","execution_count":1,"outputs":[{"output_type":"stream","text":"1.5.1\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## 2.2.1 创建`Tensor`\n\n创建一个5x3的未初始化的`Tensor`："},{"metadata":{"trusted":true},"cell_type":"code","source":"x = torch.empty(5, 3)\nprint(x)","execution_count":2,"outputs":[{"output_type":"stream","text":"tensor([[1.0445e-11, 3.0760e-41, 1.0445e-11],\n        [3.0760e-41, 1.5975e-43, 0.0000e+00],\n        [2.0039e-43, 0.0000e+00, 1.0410e-11],\n        [3.0760e-41, 1.0410e-11, 3.0760e-41],\n        [1.0410e-11, 3.0760e-41, 1.0270e-11]])\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"创建一个5x3的随机初始化的`Tensor`:\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"x = torch.rand(5, 3)\nprint(x)","execution_count":3,"outputs":[{"output_type":"stream","text":"tensor([[0.4963, 0.7682, 0.0885],\n        [0.1320, 0.3074, 0.6341],\n        [0.4901, 0.8964, 0.4556],\n        [0.6323, 0.3489, 0.4017],\n        [0.0223, 0.1689, 0.2939]])\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"创建一个5x3的long型全0的`Tensor`:\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"x = torch.zeros(5, 3, dtype=torch.long)\nprint(x)","execution_count":4,"outputs":[{"output_type":"stream","text":"tensor([[0, 0, 0],\n        [0, 0, 0],\n        [0, 0, 0],\n        [0, 0, 0],\n        [0, 0, 0]])\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"直接根据数据创建:"},{"metadata":{"trusted":true},"cell_type":"code","source":"x = torch.tensor([5.5, 3])\nprint(x)","execution_count":5,"outputs":[{"output_type":"stream","text":"tensor([5.5000, 3.0000])\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"还可以通过现有的`Tensor`来创建，此方法会默认重用输入`Tensor`的一些属性，例如数据类型，除非自定义数据类型。"},{"metadata":{"trusted":true},"cell_type":"code","source":"x = x.new_ones(5, 3, dtype=torch.float64)      # 返回的tensor默认具有相同的torch.dtype和torch.device\nprint(x)\n\nx = torch.randn_like(x, dtype=torch.float)    # 指定新的数据类型\nprint(x)                                    ","execution_count":6,"outputs":[{"output_type":"stream","text":"tensor([[1., 1., 1.],\n        [1., 1., 1.],\n        [1., 1., 1.],\n        [1., 1., 1.],\n        [1., 1., 1.]], dtype=torch.float64)\ntensor([[ 0.6035,  0.8110, -0.0451],\n        [ 0.8797,  1.0482, -0.0445],\n        [-0.7229,  2.8663, -0.5655],\n        [ 0.1604, -0.0254,  1.0739],\n        [ 2.2628, -0.9175, -0.2251]])\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"我们可以通过`shape`或者`size()`来获取`Tensor`的形状:"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(x.size())\nprint(x.shape)","execution_count":7,"outputs":[{"output_type":"stream","text":"torch.Size([5, 3])\ntorch.Size([5, 3])\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"> 注意：返回的torch.Size其实就是一个tuple, 支持所有tuple的操作。"},{"metadata":{},"cell_type":"markdown","source":"## 2.2.2 操作\n### 算术操作\n* **加法形式一**"},{"metadata":{"trusted":true},"cell_type":"code","source":"y = torch.rand(5, 3)\nprint(x + y)","execution_count":8,"outputs":[{"output_type":"stream","text":"tensor([[ 1.3967,  1.0892,  0.4369],\n        [ 1.6995,  2.0453,  0.6539],\n        [-0.1553,  3.7016, -0.3599],\n        [ 0.7536,  0.0870,  1.2274],\n        [ 2.5046, -0.1913,  0.4760]])\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"* **加法形式二**"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(torch.add(x, y))","execution_count":9,"outputs":[{"output_type":"stream","text":"tensor([[ 1.3967,  1.0892,  0.4369],\n        [ 1.6995,  2.0453,  0.6539],\n        [-0.1553,  3.7016, -0.3599],\n        [ 0.7536,  0.0870,  1.2274],\n        [ 2.5046, -0.1913,  0.4760]])\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"result = torch.empty(5, 3)\ntorch.add(x, y, out=result)\nprint(result)","execution_count":10,"outputs":[{"output_type":"stream","text":"tensor([[ 1.3967,  1.0892,  0.4369],\n        [ 1.6995,  2.0453,  0.6539],\n        [-0.1553,  3.7016, -0.3599],\n        [ 0.7536,  0.0870,  1.2274],\n        [ 2.5046, -0.1913,  0.4760]])\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"* **加法形式三、inplace**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# adds x to y\ny.add_(x)\nprint(y)","execution_count":11,"outputs":[{"output_type":"stream","text":"tensor([[ 1.3967,  1.0892,  0.4369],\n        [ 1.6995,  2.0453,  0.6539],\n        [-0.1553,  3.7016, -0.3599],\n        [ 0.7536,  0.0870,  1.2274],\n        [ 2.5046, -0.1913,  0.4760]])\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"> **注：PyTorch操作inplace版本都有后缀\"_\", 例如`x.copy_(y), x.t_()`**\n\n### 索引\n我们还可以使用类似NumPy的索引操作来访问`Tensor`的一部分，需要注意的是：**索引出来的结果与原数据共享内存，也即修改一个，另一个会跟着修改。** "},{"metadata":{"trusted":true},"cell_type":"code","source":"y = x[0, :]\ny += 1\nprint(y)\nprint(x[0, :]) # 源tensor也被改了","execution_count":12,"outputs":[{"output_type":"stream","text":"tensor([1.6035, 1.8110, 0.9549])\ntensor([1.6035, 1.8110, 0.9549])\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### 改变形状\n用`view()`来改变`Tensor`的形状："},{"metadata":{"trusted":true},"cell_type":"code","source":"y = x.view(15)\nz = x.view(-1, 5)  # -1所指的维度可以根据其他维度的值推出来\nprint(x.size(), y.size(), z.size())","execution_count":13,"outputs":[{"output_type":"stream","text":"torch.Size([5, 3]) torch.Size([15]) torch.Size([3, 5])\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"**注意`view()`返回的新tensor与源tensor共享内存，也即更改其中的一个，另外一个也会跟着改变。**"},{"metadata":{"trusted":true},"cell_type":"code","source":"x += 1\nprint(x)\nprint(y) # 也加了1","execution_count":14,"outputs":[{"output_type":"stream","text":"tensor([[2.6035, 2.8110, 1.9549],\n        [1.8797, 2.0482, 0.9555],\n        [0.2771, 3.8663, 0.4345],\n        [1.1604, 0.9746, 2.0739],\n        [3.2628, 0.0825, 0.7749]])\ntensor([2.6035, 2.8110, 1.9549, 1.8797, 2.0482, 0.9555, 0.2771, 3.8663, 0.4345,\n        1.1604, 0.9746, 2.0739, 3.2628, 0.0825, 0.7749])\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"如果不想共享内存，推荐先用`clone`创造一个副本然后再使用`view`。"},{"metadata":{"trusted":true},"cell_type":"code","source":"x_cp = x.clone().view(15)\nx -= 1\nprint(x)\nprint(x_cp)","execution_count":15,"outputs":[{"output_type":"stream","text":"tensor([[ 1.6035,  1.8110,  0.9549],\n        [ 0.8797,  1.0482, -0.0445],\n        [-0.7229,  2.8663, -0.5655],\n        [ 0.1604, -0.0254,  1.0739],\n        [ 2.2628, -0.9175, -0.2251]])\ntensor([2.6035, 2.8110, 1.9549, 1.8797, 2.0482, 0.9555, 0.2771, 3.8663, 0.4345,\n        1.1604, 0.9746, 2.0739, 3.2628, 0.0825, 0.7749])\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"另外一个常用的函数就是`item()`, 它可以将一个标量`Tensor`转换成一个Python number："},{"metadata":{"trusted":true},"cell_type":"code","source":"x = torch.randn(1)\nprint(x)\nprint(x.item())","execution_count":16,"outputs":[{"output_type":"stream","text":"tensor([2.3466])\n2.3466382026672363\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## 2.2.3 广播机制"},{"metadata":{"trusted":true},"cell_type":"code","source":"x = torch.arange(1, 3).view(1, 2)\nprint(x)\ny = torch.arange(1, 4).view(3, 1)\nprint(y)\nprint(x + y)","execution_count":17,"outputs":[{"output_type":"stream","text":"tensor([[1, 2]])\ntensor([[1],\n        [2],\n        [3]])\ntensor([[2, 3],\n        [3, 4],\n        [4, 5]])\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## 2.2.4 运算的内存开销"},{"metadata":{"trusted":true},"cell_type":"code","source":"x = torch.tensor([1, 2])\ny = torch.tensor([3, 4])\nid_before = id(y)\ny = y + x\nprint(id(y) == id_before)","execution_count":18,"outputs":[{"output_type":"stream","text":"False\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = torch.tensor([1, 2])\ny = torch.tensor([3, 4])\nid_before = id(y)\ny[:] = y + x\nprint(id(y) == id_before)","execution_count":19,"outputs":[{"output_type":"stream","text":"True\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = torch.tensor([1, 2])\ny = torch.tensor([3, 4])\nid_before = id(y)\ntorch.add(x, y, out=y) # y += x, y.add_(x)\nprint(id(y) == id_before)","execution_count":20,"outputs":[{"output_type":"stream","text":"True\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## 2.2.5 `Tensor`和NumPy相互转换\n**`numpy()`和`from_numpy()`这两个函数产生的`Tensor`和NumPy array实际是使用的相同的内存，改变其中一个时另一个也会改变！！！**\n### `Tensor`转NumPy"},{"metadata":{"trusted":true},"cell_type":"code","source":"a = torch.ones(5)\nb = a.numpy()\nprint(a, b)\n\na += 1\nprint(a, b)\nb += 1\nprint(a, b)","execution_count":21,"outputs":[{"output_type":"stream","text":"tensor([1., 1., 1., 1., 1.]) [1. 1. 1. 1. 1.]\ntensor([2., 2., 2., 2., 2.]) [2. 2. 2. 2. 2.]\ntensor([3., 3., 3., 3., 3.]) [3. 3. 3. 3. 3.]\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### NumPy数组转`Tensor`"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\na = np.ones(5)\nb = torch.from_numpy(a)\nprint(a, b)\n\na += 1\nprint(a, b)\nb += 1\nprint(a, b)","execution_count":22,"outputs":[{"output_type":"stream","text":"[1. 1. 1. 1. 1.] tensor([1., 1., 1., 1., 1.], dtype=torch.float64)\n[2. 2. 2. 2. 2.] tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n[3. 3. 3. 3. 3.] tensor([3., 3., 3., 3., 3.], dtype=torch.float64)\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"直接用`torch.tensor()`将NumPy数组转换成`Tensor`，该方法总是会进行数据拷贝，返回的`Tensor`和原来的数据不再共享内存。"},{"metadata":{"trusted":true},"cell_type":"code","source":"# 用torch.tensor()转换时不会共享内存\nc = torch.tensor(a)\na += 1\nprint(a, c)","execution_count":23,"outputs":[{"output_type":"stream","text":"[4. 4. 4. 4. 4.] tensor([3., 3., 3., 3., 3.], dtype=torch.float64)\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## 2.2.6 `Tensor` on GPU"},{"metadata":{"trusted":true},"cell_type":"code","source":"# 以下代码只有在PyTorch GPU版本上才会执行\nif torch.cuda.is_available():\n    device = torch.device(\"cuda\")          # GPU\n    y = torch.ones_like(x, device=device)  # 直接创建一个在GPU上的Tensor\n    x = x.to(device)                       # 等价于 .to(\"cuda\")\n    z = x + y\n    print(z)\n    print(z.to(\"cpu\", torch.double))       # to()还可以同时更改数据类型","execution_count":24,"outputs":[{"output_type":"stream","text":"The slowest run took 4.85 times longer than the fastest. This could mean that an intermediate result is being cached.\n19.8 µs ± 16.4 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n","name":"stdout"},{"output_type":"error","ename":"RuntimeError","evalue":"expected device cuda:0 but got device cpu","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-24-0c02dd06b68b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'timeit'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'y = torch.ones_like(x, device=device)  # 直接创建一个在GPU上的Tensor'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m                       \u001b[0;31m# 等价于 .to(\"cuda\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdouble\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m       \u001b[0;31m# to()还可以同时更改数据类型\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: expected device cuda:0 but got device cpu"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}